---
title: "Mixed Continuous and Binary Co-Primary Endpoints"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Mixed Continuous and Binary Co-Primary Endpoints}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Overview

This vignette demonstrates sample size calculation for clinical trials with two co-primary endpoints where one is continuous and one is binary. The methodology is based on Sozu et al. (2012).

```{r setup, message=FALSE, warning=FALSE}
library(twoCoprimary)
library(dplyr)
library(tidyr)
library(knitr)
```

## Background

### Clinical Context

Mixed continuous and binary co-primary endpoints are common in:

- **Alzheimer's trials**: ADAS-cog score (continuous) + CIBIC-plus impression (binary: improved/not improved)
- **Depression trials**: HAM-D score (continuous) + Response rate (binary: ≥50% reduction)
- **Cardiovascular trials**: Blood pressure change (continuous) + Target achievement (binary: BP < 140/90)
- **Pain trials**: Pain intensity on VAS (continuous) + ≥50% pain reduction (binary)
- **Diabetes trials**: HbA1c change (continuous) + Target achievement (binary: HbA1c < 7%)

### Why Mixed Endpoints?

Combining continuous and binary endpoints provides:
1. **Comprehensive assessment**: Magnitude of change (continuous) + clinical meaningfulness (binary)
2. **Regulatory relevance**: Many guidelines require both types
3. **Clinical interpretability**: Binary endpoints are easier to communicate to patients

## Statistical Framework

### Model and Assumptions

Consider a two-arm superiority trial with sample sizes n₁ (treatment) and n₂ (control).

**Endpoint 1 (Continuous)**:
$$Y_{ij1} \sim N(\mu_{j1}, \sigma^2)$$

where:
- μⱼ₁ is the population mean in group j
- σ² is the common variance across groups

**Endpoint 2 (Binary)**:
$$Y_{ij2} \sim \text{Bernoulli}(p_{j2})$$

where pⱼ₂ is the success probability in group j.

### Correlation Structure: Biserial Correlation

The correlation between a continuous variable and a binary variable requires special consideration. We use the **biserial correlation** ρ.

**Key concept**: The binary outcome Y₂ is assumed to arise from dichotomizing a latent continuous variable Y₂*:

$$Y_2 = \begin{cases} 
1 & \text{if } Y_2^* > c \\
0 & \text{if } Y_2^* \leq c
\end{cases}$$

where c is a threshold such that P(Y₂* > c) = p₂.

**Biserial correlation** ρ measures the correlation between:
- Y₁: Observed continuous variable
- Y₂*: Latent continuous variable underlying Y₂

Assume (Y₁, Y₂*) follows a bivariate normal distribution with correlation ρ. Then:

$$p_2 = P(Y_2^* > c) = 1 - \Phi\left(\frac{c - \mu_{Y_2^*}}{\sigma_{Y_2^*}}\right)$$

By standardization, we can set μ_{Y₂*} = 0 and σ_{Y₂*} = 1, giving:

$$c = \Phi^{-1}(1 - p_2)$$

**Relationship to observed data**: The biserial correlation ρ can be estimated from observed data using:

$$\hat{\rho} = \frac{\bar{Y}_{1|Y_2=1} - \bar{Y}_{1|Y_2=0}}{\sigma_{Y_1}} \times \frac{\sqrt{p_2(1-p_2)}}{\phi(\Phi^{-1}(1-p_2))}$$

where φ is the standard normal PDF.

### Hypothesis Testing

**For continuous endpoint** (k = 1):
$$H_{01}: \mu_{11} - \mu_{21} \leq 0 \text{ vs. } H_{11}: \mu_{11} - \mu_{21} > 0$$

**For binary endpoint** (k = 2):
$$H_{02}: p_{12} - p_{22} \leq 0 \text{ vs. } H_{12}: p_{12} - p_{22} > 0$$

**Co-primary endpoints** (intersection-union test):
$$H_0 = H_{01} \cup H_{02} \text{ vs. } H_1 = H_{11} \cap H_{12}$$

Reject H₀ at level α if and only if **both** H₀₁ and H₀₂ are rejected at level α.

### Test Statistics

**Continuous endpoint**:
$$Z_1 = \frac{\bar{Y}_{11} - \bar{Y}_{21}}{\sigma\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$

When σ is unknown, use the pooled sample standard deviation s.

Under H₁₁, Z₁ ~ N(μ₁, 1) where:
$$\mu_1 = \frac{\Delta_1}{\sigma\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} = \frac{\delta_1}{\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$

with standardized effect size δ₁ = Δ₁/σ.

**Binary endpoint** (several methods available):

1. **AN (Asymptotic Normal)**: Without continuity correction
$$Z_2 = \frac{\hat{p}_{12} - \hat{p}_{22}}{\sqrt{\hat{V}_2}}$$

where $\hat{V}_2 = \frac{\hat{p}_{12}(1-\hat{p}_{12})}{n_1} + \frac{\hat{p}_{22}(1-\hat{p}_{22})}{n_2}$

2. **ANc**: With continuity correction
$$Z_2 = \frac{|\hat{p}_{12} - \hat{p}_{22}| - \frac{1}{2}(\frac{1}{n_1} + \frac{1}{n_2})}{\sqrt{\hat{V}_2}}$$

3. **AS (Arcsine)**: Variance-stabilizing transformation
$$Z_2 = \frac{\arcsin(\sqrt{\hat{p}_{12}}) - \arcsin(\sqrt{\hat{p}_{22}})}{\sqrt{\frac{1}{4n_1} + \frac{1}{4n_2}}}$$

4. **ASc**: Arcsine with continuity correction

### Joint Distribution

Under H₁, the test statistics (Z₁, Z₂) asymptotically follow a bivariate normal:

$$\begin{pmatrix} Z_1 \\ Z_2 \end{pmatrix} \sim BN\left(\begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} 1 & \gamma \\ \gamma & 1 \end{pmatrix}\right)$$

The **correlation γ between test statistics** depends on the biserial correlation ρ between endpoints.

**Key challenge**: Relating ρ (correlation between Y₁ and Y₂*) to γ (correlation between Z₁ and Z₂).

Sozu et al. (2012) derived the following approximation. For balanced design (n₁ = n₂ = n):

$$\gamma \approx \frac{\rho \cdot \phi(\Phi^{-1}(1-\bar{p}_2))}{\sqrt{2\bar{p}_2(1-\bar{p}_2)}}$$

where $\bar{p}_2 = (p_{12} + p_{22})/2$ is the average success probability.

For general allocation ratio r = n₁/n₂, the correlation is weighted by sample sizes:

$$\gamma \approx \frac{\sum_{j=1,2} \frac{n_j}{n_1 + n_2} \rho_j \phi(\Phi^{-1}(1-p_{j2}))}{\sqrt{V_2} \sqrt{\bar{p}_2(1-\bar{p}_2)}}$$

### Power Calculation

The overall power is:

$$1 - \beta = P(Z_1 > z_{1-\alpha} \text{ and } Z_2 > z_{1-\alpha} \mid H_1)$$

Using the bivariate normal CDF:

$$1 - \beta = \Phi_2(-z_{1-\alpha} + \mu_1, -z_{1-\alpha} + \mu_2 \mid \gamma)$$

### Sample Size Determination

The sample size is determined by solving:

$$\Phi_2\left(-z_{1-\alpha} + \frac{\delta_1}{\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}, -z_{1-\alpha} + \frac{\Delta_2}{\sqrt{V_2}} \mid \gamma\right) = 1 - \beta$$

where:
- Δ₂ = p₁₂ - p₂₂ is the treatment effect on the binary endpoint
- V₂ depends on n₁, n₂, p₁₂, p₂₂

This equation is solved numerically for n (or n₁, n₂).

**Computational approach**:
1. Calculate individual-endpoint sample sizes
2. Calculate joint power accounting for correlation
3. Iterate until target power achieved

## Understanding c₁*/c₂*: Power Balance

The parameter c₁*/c₂* in Sozu et al. (2012) represents the **power balance** between endpoints.

**Definition**: c₁* and c₂* are the arguments in the power function:
$$1 - \beta = \Phi_2(c_1^*, c_2^* \mid \gamma)$$

where:
- c₁* = -z₁₋α + μ₁
- c₂* = -z₁₋α + μ₂

**Interpretation**:
- **c₁*/c₂* = 1**: Both endpoints have equal individual power
- **c₁*/c₂* > 1**: Continuous endpoint has higher individual power
- **c₁*/c₂* < 1**: Binary endpoint has higher individual power

**Why it matters**: The power balance affects how much correlation impacts sample size. When powers are balanced (c₁*/c₂* ≈ 1), correlation has maximum effect on sample size reduction.

## Replicating Sozu et al. (2012) Table 1

Table 1 shows sample sizes for different standardized effect sizes and probability pairs with equal individual power (c₁*/c₂* = 1).

### Table 1: Equal Individual Power (c₁*/c₂* = 1)

```{r table1_equal_power}
# Define scenarios with c₁*/c₂* = 1 (equal power)
scenarios_table1 <- data.frame(
  delta_star = c(0.070, 0.074, 0.082, 0.098, 0.145, 0.153, 0.171, 0.210),
  pT2 = c(0.55, 0.65, 0.75, 0.85, 0.60, 0.70, 0.80, 0.90),
  pC2 = c(0.50, 0.60, 0.70, 0.80, 0.50, 0.60, 0.70, 0.80)
)

rho_values <- c(0.0, 0.3, 0.5, 0.8)

results_table1 <- lapply(1:nrow(scenarios_table1), function(i) {
  scenario <- scenarios_table1[i, ]
  
  results_rho <- lapply(rho_values, function(rho) {
    result <- ss2MixedContinuousBinary(
      delta = scenario$delta_star,
      sd = 1,
      p1 = scenario$pT2,
      p2 = scenario$pC2,
      rho = rho,
      r = 1,
      alpha = 0.025,
      beta = 0.2,
      Test = "AN"
    )
    
    data.frame(
      delta_star = scenario$delta_star,
      binary_pair = paste0("(", scenario$pT2, ", ", scenario$pC2, ")"),
      rho = rho,
      n_per_group = result$n2
    )
  })
  bind_rows(results_rho)
})

table1_results <- bind_rows(results_table1)

# Reshape for display
table1_wide <- table1_results %>%
  select(delta_star, binary_pair, rho, n_per_group) %>%
  pivot_wider(
    names_from = rho,
    values_from = n_per_group,
    names_prefix = "rho_"
  )

kable(table1_wide,
      caption = "Table 1: Sample Size per Group (c₁*/c₂* = 1)",
      digits = 0,
      col.names = c("δ₁*", "(πT₂, πC₂)", "ρ=0.0", "ρ=0.3", "ρ=0.5", "ρ=0.8"))
```

**Observations**:
- Larger effect sizes require smaller sample sizes
- Higher correlation reduces sample size by ~5-7% (ρ=0.5) to ~10-11% (ρ=0.8)
- Binary endpoint probabilities closer to 0.5 require larger samples (higher variance)

### Table 1: Moderately Unequal Power (c₁*/c₂* = 1.5)

```{r table1_unequal_power_1_5}
# Scenarios with c₁*/c₂* = 1.5
scenarios_table1_1_5 <- data.frame(
  delta_star = c(0.115, 0.119, 0.129, 0.151, 0.232, 0.242, 0.266, 0.323),
  pT2 = c(0.55, 0.65, 0.75, 0.85, 0.60, 0.70, 0.80, 0.90),
  pC2 = c(0.50, 0.60, 0.70, 0.80, 0.50, 0.60, 0.70, 0.80)
)

results_table1_1_5 <- lapply(1:nrow(scenarios_table1_1_5), function(i) {
  scenario <- scenarios_table1_1_5[i, ]
  
  results_rho <- lapply(rho_values, function(rho) {
    result <- ss2MixedContinuousBinary(
      delta = scenario$delta_star,
      sd = 1,
      p1 = scenario$pT2,
      p2 = scenario$pC2,
      rho = rho,
      r = 1,
      alpha = 0.025,
      beta = 0.2,
      Test = "AN"
    )
    
    data.frame(
      delta_star = scenario$delta_star,
      binary_pair = paste0("(", scenario$pT2, ", ", scenario$pC2, ")"),
      rho = rho,
      n_per_group = result$n2
    )
  })
  bind_rows(results_rho)
})

table1_1_5_results <- bind_rows(results_table1_1_5)

table1_1_5_wide <- table1_1_5_results %>%
  select(delta_star, binary_pair, rho, n_per_group) %>%
  pivot_wider(
    names_from = rho,
    values_from = n_per_group,
    names_prefix = "rho_"
  )

kable(table1_1_5_wide,
      caption = "Table 1: Sample Size per Group (c₁*/c₂* = 1.5)",
      digits = 0,
      col.names = c("δ₁*", "(πT₂, πC₂)", "ρ=0.0", "ρ=0.3", "ρ=0.5", "ρ=0.8"))
```

**Key finding**: When c₁*/c₂* = 1.5 (continuous endpoint has higher power), correlation has slightly less impact on sample size because the binary endpoint (with lower power) dominates.

### Table 1: Highly Unequal Power (c₁*/c₂* = 3)

```{r table1_unequal_power_3}
# Scenarios with c₁*/c₂* = 3
scenarios_table1_3 <- data.frame(
  delta_star = c(0.233, 0.239, 0.258, 0.302, 0.465, 0.483, 0.532, 0.646),
  pT2 = c(0.55, 0.65, 0.75, 0.85, 0.60, 0.70, 0.80, 0.90),
  pC2 = c(0.50, 0.60, 0.70, 0.80, 0.50, 0.60, 0.70, 0.80)
)

results_table1_3 <- lapply(1:nrow(scenarios_table1_3), function(i) {
  scenario <- scenarios_table1_3[i, ]
  
  results_rho <- lapply(rho_values, function(rho) {
    result <- ss2MixedContinuousBinary(
      delta = scenario$delta_star,
      sd = 1,
      p1 = scenario$pT2,
      p2 = scenario$pC2,
      rho = rho,
      r = 1,
      alpha = 0.025,
      beta = 0.2,
      Test = "AN"
    )
    
    data.frame(
      delta_star = scenario$delta_star,
      binary_pair = paste0("(", scenario$pT2, ", ", scenario$pC2, ")"),
      rho = rho,
      n_per_group = result$n2
    )
  })
  bind_rows(results_rho)
})

table1_3_results <- bind_rows(results_table1_3)

table1_3_wide <- table1_3_results %>%
  select(delta_star, binary_pair, rho, n_per_group) %>%
  pivot_wider(
    names_from = rho,
    values_from = n_per_group,
    names_prefix = "rho_"
  )

kable(table1_3_wide,
      caption = "Table 1: Sample Size per Group (c₁*/c₂* = 3)",
      digits = 0,
      col.names = c("δ₁*", "(πT₂, πC₂)", "ρ=0.0", "ρ=0.3", "ρ=0.5", "ρ=0.8"))
```

**Key finding**: When c₁*/c₂* = 3, the binary endpoint dominates sample size, so correlation has minimal impact (< 3% reduction even at ρ = 0.8).

## Impact of Test Method

Compare different test methods for the binary endpoint:

```{r test_method_comparison}
# Fixed scenario
delta <- 0.15
p1 <- 0.65
p2 <- 0.50
rho <- 0.5

test_methods <- c("AN", "ANc", "AS", "ASc")

test_comparison <- lapply(test_methods, function(test_method) {
  result <- ss2MixedContinuousBinary(
    delta = delta,
    sd = 1,
    p1 = p1,
    p2 = p2,
    rho = rho,
    r = 1,
    alpha = 0.025,
    beta = 0.2,
    Test = test_method
  )
  
  data.frame(
    Test_method = test_method,
    n_per_group = result$n2,
    N_total = result$N
  )
})

test_comparison_table <- bind_rows(test_comparison)

kable(test_comparison_table,
      caption = "Comparison of Different Test Methods for Binary Endpoint",
      digits = 0,
      col.names = c("Test Method", "n per group", "N total"))
```

**Key findings**:
- **AN** (no continuity correction): Smallest sample size
- **ANc** (with continuity correction): Slightly larger (~1-5% increase)
- **AS** (arcsine): Similar to AN
- **ASc** (arcsine with CC): Similar to ANc

## Correlation Impact Visualization

```{r correlation_impact_plot, fig.width=8, fig.height=6}
# Fixed effect sizes, vary correlation
delta <- 0.15
p1 <- 0.65
p2 <- 0.50

rho_seq <- seq(0, 0.8, by = 0.05)

sample_sizes <- sapply(rho_seq, function(rho) {
  result <- ss2MixedContinuousBinary(
    delta = delta,
    sd = 1,
    p1 = p1,
    p2 = p2,
    rho = rho,
    r = 1,
    alpha = 0.025,
    beta = 0.2,
    Test = "AN"
  )
  result$N
})

plot(rho_seq, sample_sizes,
     type = "l", lwd = 2, col = "blue",
     xlab = "Biserial Correlation (ρ)",
     ylab = "Total Sample Size (N)",
     main = paste0("Sample Size vs Correlation\n",
                   "δ*=", delta, ", Binary: (", p1, ", ", p2, ")"),
     las = 1)
grid()

# Add reference line at rho=0
abline(h = sample_sizes[1], lty = 2, col = "gray")

# Calculate reductions
reduction_05 <- round((1 - sample_sizes[11]/sample_sizes[1]) * 100, 1)
reduction_08 <- round((1 - sample_sizes[17]/sample_sizes[1]) * 100, 1)

legend("topright",
       legend = c(
         paste0("ρ=0.5: ", reduction_05, "% reduction"),
         paste0("ρ=0.8: ", reduction_08, "% reduction")
       ),
       bty = "n")
```

## Comparison with Two Continuous Endpoints

How do mixed endpoints compare to two continuous endpoints?

```{r mixed_vs_two_continuous}
# Mixed endpoints
mixed <- ss2MixedContinuousBinary(
  delta = 0.3,
  sd = 1,
  p1 = 0.65,
  p2 = 0.50,
  rho = 0.5,
  r = 1,
  alpha = 0.025,
  beta = 0.2,
  Test = "AN"
)

# Two continuous (same effect size)
two_cont <- ss2Continuous(
  delta1 = 0.3,
  delta2 = 0.3,
  sd1 = 1,
  sd2 = 1,
  rho = 0.5,
  r = 1,
  alpha = 0.025,
  beta = 0.2,
  known_var = TRUE
)

comparison <- data.frame(
  Endpoint_types = c("Mixed (Continuous + Binary)", "Two Continuous"),
  n_per_group = c(mixed$n2, two_cont$n2),
  N_total = c(mixed$N, two_cont$N)
)

kable(comparison,
      caption = "Comparison: Mixed vs Two Continuous Endpoints (ρ=0.5)",
      digits = 0,
      col.names = c("Endpoint Types", "n per group", "N total"))
```

**Finding**: Mixed endpoints and two continuous endpoints require similar sample sizes when effect sizes are comparable.

## Clinical Application: Alzheimer's Disease Trial

Consider an Alzheimer's trial with:

1. **ADAS-cog** (continuous): Cognitive assessment scale
2. **CIBIC-plus** (binary): Clinical global impression of change (improved vs not improved)

```{r alzheimers_mixed_example}
# Parameters
alzheimers_design <- ss2MixedContinuousBinary(
  delta = 3.0,      # ADAS-cog: 3-point improvement
  sd = 10.0,        # SD = 10 points
  p1 = 0.40,        # CIBIC-plus: 40% improved (treatment)
  p2 = 0.25,        # CIBIC-plus: 25% improved (placebo)
  rho = 0.5,        # Moderate correlation
  r = 1,
  alpha = 0.025,
  beta = 0.1,       # 90% power
  Test = "AN"
)

cat("Alzheimer's Trial Design (Mixed Endpoints):\n")
cat("Sample size per group:", alzheimers_design$n2, "\n")
cat("Total sample size:", alzheimers_design$N, "\n\n")

# Compare with no correlation
alzheimers_nocorr <- ss2MixedContinuousBinary(
  delta = 3.0,
  sd = 10.0,
  p1 = 0.40,
  p2 = 0.25,
  rho = 0,  # No correlation
  r = 1,
  alpha = 0.025,
  beta = 0.1,
  Test = "AN"
)

cat("If correlation ignored (ρ=0):\n")
cat("Sample size per group:", alzheimers_nocorr$n2, "\n")
cat("Total sample size:", alzheimers_nocorr$N, "\n\n")

reduction <- alzheimers_nocorr$N - alzheimers_design$N
pct_reduction <- round(reduction / alzheimers_nocorr$N * 100, 1)

cat("Benefit of accounting for correlation:\n")
cat("Sample size reduction:", reduction, "subjects\n")
cat("Percentage reduction:", pct_reduction, "%\n")
```

## Summary

### Key Findings

1. **Correlation impact**: 5-7% reduction at ρ = 0.8, similar to other endpoint types

2. **Power balance** (c₁*/c₂*):
   - When c₁*/c₂* = 1: Equal power, correlation matters most
   - When c₁*/c₂* = 3: Binary endpoint dominates, correlation impact minimal

3. **Test method**: AN (no continuity correction) gives smallest sample size; continuity correction adds 1-5%

4. **Comparison with two continuous**: Sample sizes are similar when effect sizes are comparable

5. **Biserial correlation**: More challenging to estimate than Pearson correlation; requires understanding of latent variable structure

### Practical Recommendations

1. **Estimating ρ**: Use pilot data or historical studies; be conservative if uncertain. Biserial correlation can be estimated from observed data or specified based on clinical knowledge.

2. **Test selection**: AN is most common; use Fisher for small samples or regulatory requirements

3. **Sample size planning**: Calculate for ρ = 0 (conservative) and expected ρ (realistic)

4. **Continuous-binary relationship**: Ensure the binary endpoint conceptually has an underlying continuous latent variable

5. **Sensitivity analysis**: Assess robustness to correlation misspecification

### Challenges and Considerations

1. **Correlation estimation**: Biserial correlation is harder to estimate than Pearson correlation because it involves a latent variable

2. **Latent variable assumption**: The binary endpoint must conceptually have an underlying continuous scale (e.g., "improved" means crossing a threshold on a continuous improvement scale)

3. **Threshold specification**: The dichotomization threshold affects correlation; ensure it's clinically meaningful

4. **Asymmetric power**: Mixed endpoints often have unequal power for the two endpoints; the endpoint with lower power dominates sample size

## References

Sozu, T., Sugimoto, T., & Hamasaki, T. (2012). Sample size determination in clinical trials with multiple co-primary endpoints including mixed continuous and binary variables. *Biometrical Journal*, 54(5), 716-729.

Sozu, T., Sugimoto, T., Hamasaki, T., & Evans, S. R. (2015). *Sample Size Determination in Clinical Trials with Multiple Endpoints*. Springer.

Tate, R. F. (1954). Correlation between a discrete and a continuous variable. Point-biserial correlation. *Annals of Mathematical Statistics*, 25, 603-607.

Pearson, K. (1909). On a new method of determining correlation between a measured character A, and a character B. *Biometrika*, 7(1-2), 96-105.
