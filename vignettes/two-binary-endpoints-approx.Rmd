---
title: "Two Binary Co-Primary Endpoints (Asymptotic Methods)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Two Binary Co-Primary Endpoints (Asymptotic Methods)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Overview

This vignette demonstrates sample size calculation for clinical trials with two co-primary binary endpoints using asymptotic normal approximation methods. The methodology is based on Sozu et al. (2010).

```{r setup, message=FALSE, warning=FALSE}
library(twoCoprimary)
library(dplyr)
library(tidyr)
library(knitr)
```

## Background

### Clinical Context

Binary co-primary endpoints are common in:

- **Oncology trials**: Tumor response (CR/PR) + Symptom control (yes/no)
- **Vaccine trials**: Seroconversion (yes/no) + Clinical protection from disease (yes/no)
- **Respiratory trials**: ≥1 symptom-free day per week (yes/no) + No rescue medication use (yes/no)
- **Cardiovascular trials**: Freedom from death (yes/no) + Freedom from hospitalization (yes/no)
- **Infectious disease trials**: Clinical cure (yes/no) + Microbiological eradication (yes/no)

### When to Use Asymptotic Methods

Asymptotic (normal approximation) methods are appropriate when:

- **Sample sizes are large** (typically N > 200)
- **Probabilities are not extreme** (0.10 < p < 0.90)
- **Computational efficiency is important**

For small to medium sample sizes or extreme probabilities, use **exact methods** (see `vignette("two-binary-endpoints-exact")`).

## Statistical Framework

### Model and Assumptions

Consider a two-arm parallel-group superiority trial comparing treatment (group 1) with control (group 2). Let n₁ and n₂ denote the sample sizes in the two groups.

For subject i in group j (j = 1: treatment, j = 2: control), we observe two binary endpoints:

**Endpoint k** (k = 1, 2):
$$Y_{ijk} \sim \text{Bernoulli}(p_{jk})$$

where pⱼₖ is the success probability for endpoint k in group j.

**Joint distribution**: The bivariate distribution within each subject follows a multinomial distribution with four possible outcomes:
- (1,1): Success on both endpoints, probability π₁₁⁽ʲ⁾
- (1,0): Success on endpoint 1 only, probability π₁₀⁽ʲ⁾
- (0,1): Success on endpoint 2 only, probability π₀₁⁽ʲ⁾
- (0,0): Failure on both endpoints, probability π₀₀⁽ʲ⁾

where:
$$\sum_{a,b \in \{0,1\}} \pi_{ab}^{(j)} = 1$$

**Marginal probabilities**:
$$p_{j1} = \pi_{11}^{(j)} + \pi_{10}^{(j)}$$
$$p_{j2} = \pi_{11}^{(j)} + \pi_{01}^{(j)}$$

### Correlation Structure

The **tetrachoric correlation** measures the association between two binary variables by assuming an underlying bivariate normal distribution.

For group j, the correlation ρⱼ between endpoints 1 and 2 is defined through:

$$\pi_{11}^{(j)} = \int_{-\infty}^{\Phi^{-1}(p_{j1})} \int_{-\infty}^{\Phi^{-1}(p_{j2})} \phi_2(x, y \mid \rho_j) \, dy \, dx$$

where:
- Φ is the standard normal CDF
- φ₂(x, y | ρⱼ) is the bivariate normal PDF with correlation ρⱼ

**Practical interpretation**: ρⱼ > 0 means subjects who succeed on endpoint 1 are more likely to succeed on endpoint 2.

**Range constraints**: The correlation ρⱼ must satisfy constraints based on the marginal probabilities. The Fréchet-Hoeffding bounds are:

$$\max(0, p_{j1} + p_{j2} - 1) \leq \pi_{11}^{(j)} \leq \min(p_{j1}, p_{j2})$$

### Hypothesis Testing

We test superiority of treatment over control for both endpoints:

**For endpoint k**:
$$H_{0k}: p_{1k} - p_{2k} \leq 0 \text{ vs. } H_{1k}: p_{1k} - p_{2k} > 0$$

**For co-primary endpoints** (intersection-union test):

**Null hypothesis**: H₀ = H₀₁ ∪ H₀₂ (at least one null is true)

**Alternative hypothesis**: H₁ = H₁₁ ∩ H₁₂ (both alternatives are true)

**Decision rule**: Reject H₀ at level α if and only if **both** H₀₁ and H₀₂ are rejected at level α.

### Test Statistics

Several test statistics are available for binary endpoints. We focus on **asymptotic normal approximation** methods.

#### Method 1: Standard Normal Approximation (AN)

For endpoint k, the test statistic without continuity correction is:

$$Z_k = \frac{\hat{p}_{1k} - \hat{p}_{2k}}{\sqrt{\hat{V}_k}}$$

where:
$$\hat{V}_k = \frac{\hat{p}_{1k}(1 - \hat{p}_{1k})}{n_1} + \frac{\hat{p}_{2k}(1 - \hat{p}_{2k})}{n_2}$$

Under H₀ₖ, Zₖ asymptotically follows N(0, 1).

Under H₁ₖ with true probabilities p₁ₖ and p₂ₖ, Zₖ asymptotically follows N(μₖ, 1), where:

$$\mu_k = \frac{p_{1k} - p_{2k}}{\sqrt{V_k}}$$

with:
$$V_k = \frac{p_{1k}(1 - p_{1k})}{n_1} + \frac{p_{2k}(1 - p_{2k})}{n_2}$$

For balanced design (n₁ = n₂ = n):
$$\mu_k = \frac{p_{1k} - p_{2k}}{\sqrt{\frac{p_{1k}(1 - p_{1k}) + p_{2k}(1 - p_{2k})}{n}}}$$

#### Method 2: Normal Approximation with Continuity Correction (ANc)

To improve finite-sample performance, continuity correction can be applied:

$$Z_k = \frac{|\hat{p}_{1k} - \hat{p}_{2k}| - \frac{1}{2}\left(\frac{1}{n_1} + \frac{1}{n_2}\right)}{\sqrt{\hat{V}_k}}$$

#### Method 3: Arcsine Transformation (AS)

The arcsine-square-root transformation stabilizes variance:

$$Z_k = \frac{\arcsin(\sqrt{\hat{p}_{1k}}) - \arcsin(\sqrt{\hat{p}_{2k}})}{\sqrt{\frac{1}{4n_1} + \frac{1}{4n_2}}}$$

This transformation is variance-stabilizing: the variance of arcsin(√p) is approximately 1/(4n) regardless of p.

### Joint Distribution and Correlation

The key challenge is relating the endpoint correlation ρⱼ within subjects to the correlation γ between test statistics Z₁ and Z₂.

Under H₁, (Z₁, Z₂) asymptotically follows a bivariate normal distribution:

$$\begin{pmatrix} Z_1 \\ Z_2 \end{pmatrix} \sim BN\left(\begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} 1 & \gamma \\ \gamma & 1 \end{pmatrix}\right)$$

The correlation γ depends on ρ₁, ρ₂, and the marginal probabilities. For the standard normal approximation:

$$\gamma = \frac{\text{Cov}(Z_1, Z_2)}{\sqrt{\text{Var}(Z_1)\text{Var}(Z_2)}}$$

**Simplified calculation** (Sozu et al., 2010): Under the assumption of common correlation (ρ₁ = ρ₂ = ρ) and using the tetrachoric correlation model, γ can be approximated as:

$$\gamma \approx \frac{\sum_{j=1,2} \frac{n_j}{n_1 + n_2} \rho_j \sqrt{p_{j1}(1-p_{j1})p_{j2}(1-p_{j2})}}{\sqrt{V_1 V_2}}$$

For balanced design with common correlation:

$$\gamma \approx \frac{\rho \sqrt{[p_{11}(1-p_{11}) + p_{21}(1-p_{21})][p_{12}(1-p_{12}) + p_{22}(1-p_{22})]}}{2\sqrt{V_1 V_2}}$$

This approximation is implemented in the package functions.

### Power Calculation

The overall power (probability of rejecting both null hypotheses) is:

$$1 - \beta = P(Z_1 > z_{1-\alpha} \text{ and } Z_2 > z_{1-\alpha} \mid H_1)$$

Using the bivariate normal distribution:

$$1 - \beta = \Phi_2(-z_{1-\alpha} + \mu_1, -z_{1-\alpha} + \mu_2 \mid \gamma)$$

where Φ₂(a, b | γ) is the CDF of the standard bivariate normal distribution with correlation γ.

### Sample Size Formula

For target power 1 - β and balanced design (n₁ = n₂ = n), we solve:

$$\Phi_2\left(-z_{1-\alpha} + \frac{(p_{11} - p_{21})}{\sqrt{2V_1/n}}, -z_{1-\alpha} + \frac{(p_{12} - p_{22})}{\sqrt{2V_2/n}} \mid \gamma\right) = 1 - \beta$$

This is solved numerically for n. The total sample size is N = 2n.

**Computational approach**:
1. Calculate individual endpoint sample sizes ignoring correlation
2. Calculate power at these sample sizes accounting for correlation
3. Refine using iterative algorithm until convergence

## Replicating Sozu et al. (2010) Table III

Table III from Sozu et al. (2010) shows sample sizes for various probability combinations and correlations.

### Part A: Equal Proportions (p₁₁ = p₁₂)

When both endpoints have the same success probabilities:

```{r table3_equal_probs}
# Define scenarios for equal proportions
prob_pairs <- list(
  c(0.55, 0.50), c(0.60, 0.55), c(0.65, 0.60), c(0.70, 0.65),
  c(0.75, 0.70), c(0.80, 0.75), c(0.85, 0.80), c(0.90, 0.85)
)

rho_values <- c(0, 0.3, 0.5, 0.8)

# Calculate sample sizes
results_equal <- lapply(prob_pairs, function(probs) {
  p1 <- probs[1]
  p2 <- probs[2]
  
  results_row <- lapply(rho_values, function(rho) {
    result <- ss2BinaryApprox(
      p11 = p1, p21 = p2,
      p12 = p1, p22 = p2,
      r = 1,
      rho1 = rho, rho2 = rho,
      alpha = 0.025,
      beta = 0.2,
      Test = "AN"
    )
    result$n2
  })
  
  data.frame(
    p_treatment = p1,
    p_control = p2,
    diff = p1 - p2,
    rho_0.0 = results_row[[1]],
    rho_0.3 = results_row[[2]],
    rho_0.5 = results_row[[3]],
    rho_0.8 = results_row[[4]]
  )
})

table3_equal <- bind_rows(results_equal)

kable(table3_equal,
      caption = "Table III (Part A): Sample Size per Group for Equal Proportions",
      digits = 0,
      col.names = c("π₁ (Trt)", "π₂ (Ctrl)", "Δ", "ρ=0.0", "ρ=0.3", "ρ=0.5", "ρ=0.8"))
```

**Observations**:
- Larger effect sizes (Δ = π₁ - π₂) require smaller sample sizes
- Higher correlation reduces required sample size
- At ρ = 0.8, approximately 11% reduction compared to ρ = 0

### Part B: Unequal Proportions

When endpoints have different success probabilities:

```{r table3_unequal_probs}
# Scenarios with different effect sizes
scenarios_unequal <- list(
  list(p11 = 0.60, p21 = 0.50, p12 = 0.60, p22 = 0.50),
  list(p11 = 0.65, p21 = 0.50, p12 = 0.60, p22 = 0.50),
  list(p11 = 0.70, p21 = 0.55, p12 = 0.65, p22 = 0.55),
  list(p11 = 0.75, p21 = 0.60, p12 = 0.70, p22 = 0.60),
  list(p11 = 0.80, p21 = 0.65, p12 = 0.75, p22 = 0.65),
  list(p11 = 0.85, p21 = 0.70, p12 = 0.80, p22 = 0.70)
)

results_unequal <- lapply(scenarios_unequal, function(scenario) {
  results_row <- lapply(rho_values, function(rho) {
    result <- ss2BinaryApprox(
      p11 = scenario$p11, p21 = scenario$p21,
      p12 = scenario$p12, p22 = scenario$p22,
      r = 1,
      rho1 = rho, rho2 = rho,
      alpha = 0.025,
      beta = 0.2,
      Test = "AN"
    )
    result$n2
  })
  
  data.frame(
    Endpoint1 = paste0(scenario$p11, " vs ", scenario$p21),
    Endpoint2 = paste0(scenario$p12, " vs ", scenario$p22),
    diff1 = scenario$p11 - scenario$p21,
    diff2 = scenario$p12 - scenario$p22,
    rho_0.0 = results_row[[1]],
    rho_0.3 = results_row[[2]],
    rho_0.5 = results_row[[3]],
    rho_0.8 = results_row[[4]]
  )
})

table3_unequal <- bind_rows(results_unequal)

kable(table3_unequal,
      caption = "Table III (Part B): Sample Size per Group for Unequal Proportions",
      digits = 0,
      col.names = c("Endpoint 1", "Endpoint 2", "Δ₁", "Δ₂", 
                    "ρ=0.0", "ρ=0.3", "ρ=0.5", "ρ=0.8"))
```

**Key finding**: When effect sizes differ across endpoints, the endpoint with smaller effect size dominates the sample size requirement.

### Comparison with Single-Endpoint Sample Sizes

How do co-primary designs compare to single-endpoint designs?

```{r comparison_single}
# Calculate sample sizes for individual endpoints
single_endpoint_comparison <- lapply(prob_pairs, function(probs) {
  p1 <- probs[1]
  p2 <- probs[2]
  
  # Single endpoint
  single <- ss1BinaryApprox(
    p1 = p1, p2 = p2,
    r = 1,
    alpha = 0.025,
    beta = 0.2,
    Test = "AN"
  )
  
  # Co-primary at rho=0
  coprimary <- ss2BinaryApprox(
    p11 = p1, p21 = p2,
    p12 = p1, p22 = p2,
    r = 1,
    rho1 = 0, rho2 = 0,
    alpha = 0.025,
    beta = 0.2,
    Test = "AN"
  )
  
  data.frame(
    p1 = p1,
    p2 = p2,
    Single_EP = single$n2,
    Coprimary_rho0 = coprimary$n2,
    Ratio = round(coprimary$n2 / single$n2, 2)
  )
})

comparison_table <- bind_rows(single_endpoint_comparison)

kable(comparison_table,
      caption = "Comparison: Single Endpoint vs Co-Primary (ρ=0)",
      digits = 2,
      col.names = c("π₁", "π₂", "Single EP", "Co-primary", "Ratio"))
```

**Interpretation**: Co-primary design with ρ = 0 requires approximately the same sample size as a single endpoint design for the same effect size. The "cost" of co-primacy is in the reduced power (product of individual powers).

## Power Verification

Let's verify that the calculated sample sizes achieve the target power:

```{r power_verification}
# Select representative scenarios
test_scenarios <- data.frame(
  p11 = c(0.70, 0.75, 0.65, 0.80),
  p21 = c(0.60, 0.70, 0.50, 0.65),
  p12 = c(0.70, 0.75, 0.60, 0.75),
  p22 = c(0.60, 0.70, 0.50, 0.65),
  rho = c(0.3, 0.5, 0.5, 0.8)
)

power_results <- lapply(1:nrow(test_scenarios), function(i) {
  scenario <- test_scenarios[i, ]
  
  # Get sample size
  ss <- ss2BinaryApprox(
    p11 = scenario$p11, p21 = scenario$p21,
    p12 = scenario$p12, p22 = scenario$p22,
    r = 1,
    rho1 = scenario$rho, rho2 = scenario$rho,
    alpha = 0.025,
    beta = 0.2,
    Test = "AN"
  )
  
  # Calculate power
  power <- power2BinaryApprox(
    n1 = ss$n1, n2 = ss$n2,
    p11 = scenario$p11, p21 = scenario$p21,
    p12 = scenario$p12, p22 = scenario$p22,
    rho1 = scenario$rho, rho2 = scenario$rho,
    alpha = 0.025,
    Test = "AN"
  )
  
  data.frame(
    EP1 = paste0(scenario$p11, " vs ", scenario$p21),
    EP2 = paste0(scenario$p12, " vs ", scenario$p22),
    rho = scenario$rho,
    n_per_group = ss$n2,
    N_total = ss$N,
    power_EP1 = power$power1,
    power_EP2 = power$power2,
    power_both = power$powerCoprimary
  )
})

power_check <- bind_rows(power_results)

kable(power_check,
      caption = "Power Verification (Target: 0.80)",
      digits = 3,
      col.names = c("Endpoint 1", "Endpoint 2", "ρ", "n per group", 
                    "N total", "Power EP1", "Power EP2", "Power Both"))
```

**Verification**: All scenarios achieve the target power of 0.80 for both endpoints.

## Impact of Correlation: Detailed Analysis

### Correlation Effect on Sample Size

```{r correlation_impact_plot, fig.width=8, fig.height=6}
# Fixed scenario: moderate effect sizes
p11 <- 0.70
p21 <- 0.55
p12 <- 0.65
p22 <- 0.50

rho_seq <- seq(0, 0.8, by = 0.05)

sample_sizes <- sapply(rho_seq, function(rho) {
  result <- ss2BinaryApprox(
    p11 = p11, p21 = p21,
    p12 = p12, p22 = p22,
    r = 1,
    rho1 = rho, rho2 = rho,
    alpha = 0.025,
    beta = 0.2,
    Test = "AN"
  )
  result$N
})

plot(rho_seq, sample_sizes,
     type = "l", lwd = 2, col = "darkblue",
     xlab = "Correlation (ρ)",
     ylab = "Total Sample Size (N)",
     main = paste0("Sample Size vs Correlation\n",
                   "EP1: ", p11, " vs ", p21, "; EP2: ", p12, " vs ", p22),
     las = 1)
grid()

# Add reference points
points(0, sample_sizes[1], pch = 19, col = "red", cex = 1.2)
points(0.5, sample_sizes[11], pch = 19, col = "blue", cex = 1.2)

# Calculate reductions
reduction_05 <- round((1 - sample_sizes[11]/sample_sizes[1]) * 100, 1)
reduction_08 <- round((1 - sample_sizes[17]/sample_sizes[1]) * 100, 1)

legend("topright",
       legend = c(
         paste0("ρ=0: N=", sample_sizes[1]),
         paste0("ρ=0.5: ", reduction_05, "% reduction"),
         paste0("ρ=0.8: ", reduction_08, "% reduction")
       ),
       bty = "n", cex = 0.9)
```

## Unbalanced Allocation

### Impact of Allocation Ratio

```{r unbalanced_allocation}
# Compare different allocation ratios
allocation_ratios <- c(1, 1.5, 2, 3)

unbalanced_results <- lapply(allocation_ratios, function(r) {
  result <- ss2BinaryApprox(
    p11 = 0.70, p21 = 0.55,
    p12 = 0.65, p22 = 0.50,
    r = r,
    rho1 = 0.5, rho2 = 0.5,
    alpha = 0.025,
    beta = 0.2,
    Test = "AN"
  )
  
  data.frame(
    Ratio = paste0(r, ":1"),
    n_treatment = result$n1,
    n_control = result$n2,
    N_total = result$N
  )
})

unbalanced_table <- bind_rows(unbalanced_results)

# Calculate percentage increase
unbalanced_table$Increase_pct <- round((unbalanced_table$N_total / unbalanced_table$N_total[1] - 1) * 100, 1)
unbalanced_table$Increase_pct[1] <- 0

kable(unbalanced_table,
      caption = "Impact of Allocation Ratio on Sample Size",
      col.names = c("Allocation", "n (Trt)", "n (Ctrl)", "N (Total)", "% Increase"))
```

**Finding**: 2:1 allocation increases total sample size by approximately 12% compared to balanced allocation.

## Clinical Application: Vaccine Trial

Consider a vaccine trial with two co-primary endpoints:
1. **Seroconversion**: Antibody level above threshold
2. **Clinical protection**: No symptomatic disease

```{r vaccine_example}
# Design parameters
# Expected rates with vaccine: 85% seroconversion, 70% protection
# Expected rates with placebo: 70% seroconversion, 50% protection
# Correlation: moderate (ρ = 0.5)

vaccine_design <- ss2BinaryApprox(
  p11 = 0.85,           # Seroconversion (vaccine)
  p21 = 0.70,           # Seroconversion (placebo)
  p12 = 0.70,           # Protection (vaccine)
  p22 = 0.50,           # Protection (placebo)
  r = 1,
  rho1 = 0.5, rho2 = 0.5,
  alpha = 0.025,
  beta = 0.1,            # 90% power
  Test = "AN"
)

cat("Vaccine Trial Design:\n")
cat("Sample size per group:", vaccine_design$n2, "\n")
cat("Total sample size:", vaccine_design$N, "\n\n")

# Verify power
vaccine_power <- power2BinaryApprox(
  n1 = vaccine_design$n1,
  n2 = vaccine_design$n2,
  p11 = 0.85, p21 = 0.70,
  p12 = 0.70, p22 = 0.50,
  rho1 = 0.5, rho2 = 0.5,
  alpha = 0.025,
  Test = "AN"
)

cat("Power verification:\n")
cat("Power for seroconversion:", round(vaccine_power$power1, 3), "\n")
cat("Power for protection:", round(vaccine_power$power2, 3), "\n")
cat("Power for both (co-primary):", round(vaccine_power$powerCoprimary, 3), "\n\n")

# Compare with ignoring correlation
vaccine_no_corr <- ss2BinaryApprox(
  p11 = 0.85, p21 = 0.70,
  p12 = 0.70, p22 = 0.50,
  r = 1,
  rho1 = 0, rho2 = 0,
  alpha = 0.025,
  beta = 0.1,
  Test = "AN"
)

cat("If correlation ignored (ρ=0):\n")
cat("Sample size per group:", vaccine_no_corr$n2, "\n")
cat("Total sample size:", vaccine_no_corr$N, "\n\n")

reduction <- vaccine_no_corr$N - vaccine_design$N
pct_reduction <- round(reduction / vaccine_no_corr$N * 100, 1)

cat("Benefit of accounting for correlation:\n")
cat("Sample size reduction:", reduction, "subjects\n")
cat("Percentage reduction:", pct_reduction, "%\n")
```

## Summary

### Key Findings

1. **Correlation impact**: ~5-7% reduction at ρ = 0.5, ~10-11% at ρ = 0.8

2. **Effect size asymmetry**: When effect sizes differ, the endpoint with smaller effect dominates sample size requirements

3. **Allocation ratio**: 2:1 allocation increases total N by approximately 12%

4. **Method comparison**: AN (no continuity correction) gives smallest sample size; continuity correction adds 1-5%

5. **Asymptotic validity**: These methods are appropriate for N > 200 and 0.1 < p < 0.9

### Practical Recommendations

1. **Estimate correlation**: Use pilot data or historical information; be conservative if uncertain

2. **Consider asymmetric effects**: Use actual effect sizes rather than assuming equality

3. **Balanced allocation**: Generally more efficient unless practical constraints require otherwise

4. **Test method selection**: AN is most common; use continuity correction for added conservatism

5. **Sample size sensitivity**: Calculate for range of plausible correlations and effect sizes

6. **Consider exact methods**: For N < 200 or extreme probabilities, use exact methods instead

### When to Use Exact Methods Instead

Use exact methods (`ss2BinaryExact`) when:
- Small to medium sample sizes (N < 200)
- Extreme probabilities (p < 0.1 or p > 0.9)
- Strict Type I error control required
- Regulatory preference for exact tests

## References

Sozu, T., Sugimoto, T., & Hamasaki, T. (2010). Sample size determination in clinical trials with multiple co-primary binary endpoints. *Statistics in Medicine*, 29(21), 2169-2179.

Sozu, T., Sugimoto, T., Hamasaki, T., & Evans, S. R. (2015). *Sample Size Determination in Clinical Trials with Multiple Endpoints*. Springer.

Guo, J. H., & Luh, W. M. (2009). An efficient method for computing the power of the tetrachoric correlation with binary and ordinal variables. *Computational Statistics & Data Analysis*, 53(12), 4434-4446.
