---
title: "Two Continuous Co-Primary Endpoints"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Two Continuous Co-Primary Endpoints}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Overview

This vignette demonstrates sample size calculation and power analysis for clinical trials with two co-primary continuous endpoints using asymptotic normal approximation methods. The methodology is based on Sozu et al. (2011).

```{r setup, message=FALSE, warning=FALSE}
library(twoCoprimary)
library(dplyr)
library(tidyr)
library(knitr)
```

## Background and Motivation

### What are Co-Primary Endpoints?

In clinical trials, **co-primary endpoints** require demonstrating statistically significant treatment effects on **all** endpoints simultaneously. Unlike multiple primary endpoints (where success on any one endpoint is sufficient), co-primary endpoints require:

1. **Rejecting all null hypotheses** at level α
2. **No multiplicity adjustment** needed for Type I error control
3. **Correlation consideration** can improve efficiency

### Clinical Examples

Co-primary continuous endpoints are common in:

- **Alzheimer's disease trials**: Cognitive function (ADAS-cog) + Clinical global impression (CIBIC-plus)
- **Cardiovascular trials**: Systolic blood pressure + Diastolic blood pressure
- **Depression trials**: HAM-D + MADRS scores
- **Pain trials**: Pain intensity + Physical function
- **Pulmonary trials**: FEV₁ + FVC (Forced Vital Capacity)

## Statistical Framework

### Model and Assumptions

Consider a two-arm parallel-group superiority trial comparing treatment (group 1) with control (group 2). Let n₁ and n₂ denote the sample sizes in the two groups, and define the allocation ratio κ = n₁/n₂.

For subject i in group j (j = 1: treatment, j = 2: control), we observe two continuous endpoints:

**Endpoint k** (k = 1, 2): 
$$Y_{ijk} \sim N(\mu_{jk}, \sigma_k^2)$$

where:
- μⱼₖ is the population mean for endpoint k in group j
- σₖ² is the common variance for endpoint k across both groups

**Within-subject correlation**: The two endpoints are correlated within each subject:
$$\text{Cor}(Y_{ij1}, Y_{ij2}) = \rho_j$$

We assume common correlation across groups: ρ₁ = ρ₂ = ρ.

### Effect Size Parameterization

The treatment effect for endpoint k is measured by:

**Absolute difference**: Δₖ = μ₁ₖ - μ₂ₖ

**Standardized effect size**: δₖ = Δₖ / σₖ

The standardized effect size is preferred as it is scale-free and facilitates comparison across studies.

### Hypothesis Testing

We test superiority of treatment over control for both endpoints:

**For endpoint k**:
$$H_{0k}: \mu_{1k} - \mu_{2k} \leq 0 \text{ vs. } H_{1k}: \mu_{1k} - \mu_{2k} > 0$$

**For co-primary endpoints**, we use the intersection-union test (IUT):

**Null hypothesis**: H₀ = H₀₁ ∪ H₀₂ (at least one null is true)

**Alternative hypothesis**: H₁ = H₁₁ ∩ H₁₂ (both alternatives are true)

**Decision rule**: Reject H₀ at level α if and only if **both** H₀₁ and H₀₂ are rejected at level α.

### Test Statistics

#### Case 1: Known Variance

When variances σ₁² and σ₂² are known (or estimated from large external data), the test statistic for endpoint k is:

$$Z_k = \frac{\bar{Y}_{1k} - \bar{Y}_{2k}}{\sigma_k\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$

Under H₀ₖ, Zₖ ~ N(0, 1).

Under H₁ₖ with true effect δₖ, Zₖ ~ N(μₖ, 1), where:

$$\mu_k = \frac{\delta_k}{\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} = \delta_k \sqrt{\frac{n_1 n_2}{n_1 + n_2}}$$

For balanced design (κ = 1, n₁ = n₂ = n):
$$\mu_k = \delta_k \sqrt{\frac{n}{2}}$$

#### Case 2: Unknown Variance

When variances are unknown, we use the pooled t-test. The test statistic is:

$$T_k = \frac{\bar{Y}_{1k} - \bar{Y}_{2k}}{S_k\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$

where Sₖ² is the pooled sample variance:

$$S_k^2 = \frac{(n_1 - 1)S_{1k}^2 + (n_2 - 1)S_{2k}^2}{n_1 + n_2 - 2}$$

Under H₀ₖ, Tₖ ~ t(n₁ + n₂ - 2).

For large samples, Tₖ approximately follows the same distribution as Zₖ, so we use the normal approximation.

### Joint Distribution of Test Statistics

The key to co-primary endpoint analysis is the **joint distribution** of (Z₁, Z₂).

Under H₁, (Z₁, Z₂) follows a bivariate normal distribution:

$$\begin{pmatrix} Z_1 \\ Z_2 \end{pmatrix} \sim BN\left(\begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}\right)$$

**Important**: The correlation ρ between endpoints equals the correlation between test statistics because both endpoints are standardized identically within each group.

### Power Calculation

The overall power (probability of rejecting both null hypotheses) is:

$$1 - \beta = P(Z_1 > z_{1-\alpha} \text{ and } Z_2 > z_{1-\alpha} \mid H_1)$$

where z₁₋α is the (1-α)-th quantile of the standard normal distribution.

Using properties of bivariate normal distribution:

$$1 - \beta = \Phi_2(-z_{1-\alpha} + \mu_1, -z_{1-\alpha} + \mu_2 \mid \rho)$$

where Φ₂(a, b | ρ) is the bivariate normal cumulative distribution function:

$$\Phi_2(a, b \mid \rho) = \int_{-\infty}^{a} \int_{-\infty}^{b} \phi_2(x, y \mid \rho) \, dy \, dx$$

with φ₂(x, y | ρ) being the bivariate standard normal density function with correlation ρ.

### Sample Size Formula

For a target power 1 - β, we solve for n (assuming balanced design):

$$\Phi_2\left(-z_{1-\alpha} + \delta_1\sqrt{\frac{n}{2}}, -z_{1-\alpha} + \delta_2\sqrt{\frac{n}{2}} \mid \rho\right) = 1 - \beta$$

This equation is solved numerically. The sample size per group is:

$$n = n_1 = n_2$$

Total sample size: N = 2n.

For unbalanced design with allocation ratio r = n₁/n₂:

$$\mu_k = \delta_k\sqrt{\frac{n_2 \cdot r \cdot n_2}{r \cdot n_2 + n_2}} = \delta_k\sqrt{\frac{r \cdot n_2}{1 + r}}$$

### Impact of Correlation

The correlation ρ has a substantial impact on required sample size:

**Positive correlation** (ρ > 0):
- Test statistics tend to be concordant
- P(Z₁ > c and Z₂ > c) increases
- **Sample size decreases**

**Zero correlation** (ρ = 0):
- Test statistics are independent
- Baseline scenario
- P(Z₁ > c and Z₂ > c) = P(Z₁ > c) × P(Z₂ > c)

**Negative correlation** (ρ < 0):
- Test statistics tend to be discordant
- P(Z₁ > c and Z₂ > c) decreases
- **Sample size increases**

**Quantitative impact**: For ρ = 0.5, sample size reduces by approximately 5-7% compared to ρ = 0. For ρ = 0.8, reduction is approximately 10-12%.

## Sample Size Calculation Examples

### Example 1: Balanced Design with Equal Effect Sizes

```{r example1}
# Design parameters
result1 <- ss2Continuous(
  delta1 = 0.5,     # Standardized effect size for endpoint 1
  delta2 = 0.5,     # Standardized effect size for endpoint 2
  sd1 = 1,          # Standard deviation for endpoint 1
  sd2 = 1,          # Standard deviation for endpoint 2
  rho = 0.5,        # Correlation between endpoints
  r = 1,            # Balanced allocation
  alpha = 0.025,    # One-sided Type I error rate
  beta = 0.2,       # Type II error rate (80% power)
  known_var = TRUE  # Variances are known
)

print(result1)
```

### Example 2: Impact of Correlation

```{r example2_correlation}
# Calculate sample sizes at different correlation levels
rho_values <- c(0, 0.3, 0.5, 0.8)

results_rho <- sapply(rho_values, function(rho) {
  result <- ss2Continuous(
    delta1 = 0.5,
    delta2 = 0.5,
    sd1 = 1,
    sd2 = 1,
    rho = rho,
    r = 1,
    alpha = 0.025,
    beta = 0.2,
    known_var = TRUE
  )
  result$N
})

correlation_table <- data.frame(
  Correlation = rho_values,
  Total_N = results_rho,
  Reduction = c("Baseline", 
                paste0(round((1 - results_rho[2]/results_rho[1]) * 100, 1), "%"),
                paste0(round((1 - results_rho[3]/results_rho[1]) * 100, 1), "%"),
                paste0(round((1 - results_rho[4]/results_rho[1]) * 100, 1), "%"))
)

kable(correlation_table,
      caption = "Impact of Correlation on Sample Size (δ₁ = δ₂ = 0.5)",
      col.names = c("ρ", "N", "Reduction vs ρ=0"))
```

**Key finding**: At ρ = 0.8, approximately 11% reduction in sample size compared to ρ = 0.

### Example 3: Unequal Effect Sizes

```{r example3_unequal}
# When effect sizes differ
result3 <- ss2Continuous(
  delta1 = 0.6,     # Larger effect for endpoint 1
  delta2 = 0.4,     # Smaller effect for endpoint 2
  sd1 = 1,
  sd2 = 1,
  rho = 0.5,
  r = 1,
  alpha = 0.025,
  beta = 0.2,
  known_var = TRUE
)

print(result3)

# Compare with equal effect sizes (average)
result3_equal <- ss2Continuous(
  delta1 = 0.5,
  delta2 = 0.5,
  sd1 = 1,
  sd2 = 1,
  rho = 0.5,
  r = 1,
  alpha = 0.025,
  beta = 0.2,
  known_var = TRUE
)

cat("\nComparison:\n")
cat("Unequal effects (0.6, 0.4): N =", result3$N, "\n")
cat("Equal effects (0.5, 0.5): N =", result3_equal$N, "\n")
cat("Difference:", result3$N - result3_equal$N, "subjects\n")
```

**Key finding**: Unequal effect sizes require larger sample sizes. The endpoint with smaller effect size dominates the sample size requirement.

### Example 4: Unbalanced Allocation

```{r example4_unbalanced}
# 2:1 allocation (treatment:control)
result4 <- ss2Continuous(
  delta1 = 0.5,
  delta2 = 0.5,
  sd1 = 1,
  sd2 = 1,
  rho = 0.5,
  r = 2,            # 2:1 allocation
  alpha = 0.025,
  beta = 0.2,
  known_var = TRUE
)

print(result4)

# Compare with balanced allocation
result4_balanced <- ss2Continuous(
  delta1 = 0.5,
  delta2 = 0.5,
  sd1 = 1,
  sd2 = 1,
  rho = 0.5,
  r = 1,
  alpha = 0.025,
  beta = 0.2,
  known_var = TRUE
)

cat("\nComparison:\n")
cat("Balanced (1:1): N =", result4_balanced$N, "\n")
cat("Unbalanced (2:1): N =", result4$N, "\n")
cat("Increase:", round((result4$N / result4_balanced$N - 1) * 100, 1), "%\n")
```

**Key finding**: Unbalanced allocation increases total sample size by approximately 12% for 2:1 allocation.

### Example 5: Different Standard Deviations

```{r example5_different_sd}
# Endpoints with different variability
result5 <- ss2Continuous(
  delta1 = 0.5,     # Same effect size
  delta2 = 0.5,
  sd1 = 1,          # Endpoint 1: lower variability
  sd2 = 2,          # Endpoint 2: higher variability
  rho = 0.5,
  r = 1,
  alpha = 0.025,
  beta = 0.2,
  known_var = TRUE
)

print(result5)

# Note: With different SDs, standardized effects differ
# Endpoint 1: Δ₁ = 0.5 × 1 = 0.5
# Endpoint 2: Δ₂ = 0.5 × 2 = 1.0
# But relative to their variability, both have δ = 0.5
```

## Power Verification

Let's verify that the calculated sample size achieves the target power:

```{r power_verification}
# Calculate sample size
ss_result <- ss2Continuous(
  delta1 = 0.5,
  delta2 = 0.5,
  sd1 = 1,
  sd2 = 1,
  rho = 0.5,
  r = 1,
  alpha = 0.025,
  beta = 0.2,
  known_var = TRUE
)

# Verify power
power_result <- power2Continuous(
  n1 = ss_result$n1,
  n2 = ss_result$n2,
  delta1 = 0.5,
  delta2 = 0.5,
  sd1 = 1,
  sd2 = 1,
  rho = 0.5,
  alpha = 0.025,
  known_var = TRUE
)

cat("Sample size calculation:\n")
cat("  n per group:", ss_result$n2, "\n")
cat("  Total N:", ss_result$N, "\n\n")

cat("Power verification:\n")
cat("  Target power: 0.80\n")
cat("  Achieved power (both endpoints):", round(power_result$powerCoprimary, 4), "\n")
cat("  Power for endpoint 1:", round(power_result$power1, 4), "\n")
cat("  Power for endpoint 2:", round(power_result$power2, 4), "\n")
```

## Visualization

### Power vs Sample Size Curves

```{r power_curves, fig.width=8, fig.height=6}
# Sample size range
n_seq <- seq(50, 200, by = 5)

# Calculate power for different sample sizes
power_by_n <- sapply(n_seq, function(n) {
  pw <- power2Continuous(
    n1 = n,
    n2 = n,
    delta1 = 0.5,
    delta2 = 0.5,
    sd1 = 1,
    sd2 = 1,
    rho = 0.5,
    alpha = 0.025,
    known_var = TRUE
  )
  c(pw$power1, pw$power2, pw$powerCoprimary)
})

# Plot
plot(n_seq, power_by_n[3, ],
     type = "l", lwd = 2, col = "blue",
     xlab = "Sample Size per Group (n)",
     ylab = "Power",
     main = "Power vs Sample Size (δ₁ = δ₂ = 0.5, ρ = 0.5)",
     ylim = c(0, 1),
     las = 1)
lines(n_seq, power_by_n[1, ], lty = 2, col = "red", lwd = 2)
lines(n_seq, power_by_n[2, ], lty = 2, col = "green", lwd = 2)
abline(h = 0.8, lty = 3, col = "gray")
abline(v = ss_result$n2, lty = 3, col = "gray")

legend("bottomright",
       legend = c("Both endpoints (co-primary)", "Endpoint 1", "Endpoint 2", "Target power = 0.80"),
       col = c("blue", "red", "green", "gray"),
       lty = c(1, 2, 2, 3),
       lwd = c(2, 2, 2, 1),
       bty = "n")

grid()
```

### Impact of Correlation on Sample Size

```{r correlation_impact_plot, fig.width=8, fig.height=6}
# Correlation range
rho_seq <- seq(-0.5, 0.9, by = 0.05)

# Calculate sample size for each correlation
n_by_rho <- sapply(rho_seq, function(rho) {
  if (rho < -0.5 || rho > 0.95) return(NA)
  result <- ss2Continuous(
    delta1 = 0.5,
    delta2 = 0.5,
    sd1 = 1,
    sd2 = 1,
    rho = rho,
    r = 1,
    alpha = 0.025,
    beta = 0.2,
    known_var = TRUE
  )
  result$N
})

# Plot
plot(rho_seq, n_by_rho,
     type = "l", lwd = 2, col = "darkblue",
     xlab = "Correlation (ρ)",
     ylab = "Total Sample Size (N)",
     main = "Sample Size vs Correlation (δ₁ = δ₂ = 0.5)",
     las = 1)
abline(v = 0, lty = 2, col = "gray")
grid()

# Highlight key points
points(0, n_by_rho[rho_seq == 0], pch = 19, col = "red", cex = 1.5)
points(0.5, n_by_rho[rho_seq == 0.5], pch = 19, col = "blue", cex = 1.5)

text(0, n_by_rho[rho_seq == 0], 
     paste0("ρ=0: N=", round(n_by_rho[rho_seq == 0])),
     pos = 4, offset = 0.5)
text(0.5, n_by_rho[rho_seq == 0.5], 
     paste0("ρ=0.5: N=", round(n_by_rho[rho_seq == 0.5])),
     pos = 4, offset = 0.5)
```

**Key insight**: Sample size is highly sensitive to correlation. Accounting for correlation is essential for efficient trial design.

## Clinical Application: Cardiovascular Trial

Consider a cardiovascular trial evaluating:
1. **Systolic blood pressure** (SBP) reduction
2. **Diastolic blood pressure** (DBP) reduction

```{r cardiovascular_example}
# Design parameters based on literature
# Expected reductions: SBP -10 mmHg, DBP -6 mmHg
# Standard deviations: SBP 15 mmHg, DBP 10 mmHg
# Correlation between SBP and DBP: typically 0.6-0.7

cv_design <- ss2Continuous(
  delta1 = 10 / 15,     # SBP: Δ/σ = 10/15 ≈ 0.67
  delta2 = 6 / 10,      # DBP: Δ/σ = 6/10 = 0.60
  sd1 = 15,
  sd2 = 10,
  rho = 0.65,           # Moderate-high correlation
  r = 1,
  alpha = 0.025,
  beta = 0.2,           # 80% power (lighter calculation)
  known_var = TRUE      # Known variance for faster calculation
)

cat("Cardiovascular Trial Design:\n")
cat("Sample size per group:", cv_design$n2, "\n")
cat("Total sample size:", cv_design$N, "\n\n")

# Compare with ignoring correlation
cv_no_corr <- ss2Continuous(
  delta1 = 10 / 15,
  delta2 = 6 / 10,
  sd1 = 15,
  sd2 = 10,
  rho = 0,              # Assume independence
  r = 1,
  alpha = 0.025,
  beta = 0.2,
  known_var = TRUE
)

cat("If correlation ignored (ρ=0):\n")
cat("Sample size per group:", cv_no_corr$n2, "\n")
cat("Total sample size:", cv_no_corr$N, "\n\n")

reduction <- cv_no_corr$N - cv_design$N
pct_reduction <- round(reduction / cv_no_corr$N * 100, 1)

cat("Benefit of accounting for correlation:\n")
cat("Sample size reduction:", reduction, "subjects\n")
cat("Percentage reduction:", pct_reduction, "%\n")
```

## Summary

### Key Findings

1. **Correlation impact**: For ρ = 0.5, sample size reduces by 5-7%. For ρ = 0.8, reduction is 10-12%.

2. **Unequal effect sizes**: The endpoint with smaller effect size dominates sample size requirements.

3. **Unbalanced allocation**: 2:1 allocation increases total sample size by approximately 12% compared to balanced allocation.

4. **Known vs unknown variance**: When variance is unknown, t-distribution is used, but for large samples, normal approximation is adequate.

5. **Individual vs joint power**: Individual endpoint power is higher than joint power. For balanced designs with equal effects, individual power ≈ √(joint power).

### Practical Recommendations

1. **Estimate correlation carefully**: Use pilot data or historical information. Conservative estimation (lower ρ) provides safety margin.

2. **Consider unequal effects**: If endpoints have different effect sizes, use the actual values rather than assuming equality.

3. **Balanced allocation preferred**: Unless there are strong ethical or practical reasons, balanced allocation is more efficient.

4. **Sensitivity analysis**: Calculate sample size for range of plausible correlation values.

5. **Verify power**: Always verify that calculated sample size achieves target power using power calculation functions.

## References

Sozu, T., Sugimoto, T., & Hamasaki, T. (2011). Sample size determination in superiority clinical trials with multiple co-primary continuous endpoints. *Japanese Journal of Biometrics*, 32(1), 15-31.

Sozu, T., Sugimoto, T., Hamasaki, T., & Evans, S. R. (2015). *Sample Size Determination in Clinical Trials with Multiple Endpoints*. Springer.

Senn, S., & Bretz, F. (2007). Power and sample size when multiple endpoints are considered. *Pharmaceutical Statistics*, 6(3), 161-170.
