---
title: "Two Binary Co-Primary Endpoints (Exact Methods)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Two Binary Co-Primary Endpoints (Exact Methods)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Overview

This vignette demonstrates exact sample size calculation for clinical trials with two co-primary binary endpoints. The methodology is based on Homma and Yoshida (2025).

```{r setup, message=FALSE, warning=FALSE}
library(twoCoprimary)
library(dplyr)
library(tidyr)
library(knitr)
```

## Background

### When to Use Exact Methods

Exact methods are recommended when:

- **Small to medium sample sizes** (N < 200)
- **Extreme probabilities** (p < 0.10 or p > 0.90)
- **Strict Type I error control** is required
- **Regulatory requirements** for exact inference

Asymptotic methods may not maintain the nominal Type I error rate in these situations, leading to either overly conservative or anti-conservative tests.

### Advantages of Exact Methods

1. **Accurate Type I error control**: Exact tests guarantee α ≤ nominal level
2. **Better small-sample performance**: No reliance on asymptotic approximations
3. **Valid for extreme probabilities**: No restrictions on p values
4. **Regulatory acceptance**: Often preferred by regulatory agencies

### Disadvantages

1. **Computational intensity**: Requires enumeration of possible outcomes
2. **Conservatism**: Discrete nature of tests can lead to conservatism
3. **Implementation complexity**: More complex than asymptotic methods

## Statistical Framework

### Model and Assumptions

Consider a two-arm parallel-group superiority trial comparing treatment (group 1) with control (group 2). Let n₁ and n₂ denote the sample sizes.

For subject i in group j (j = 1: treatment, j = 2: control), we observe two binary endpoints:

**Endpoint k** (k = 1, 2):
$$Y_{ijk} \sim \text{Bernoulli}(p_{jk})$$

**Joint distribution within subjects**: The bivariate outcome follows a multinomial distribution with four cells:
- π₁₁⁽ʲ⁾: P(Y₁ = 1, Y₂ = 1) in group j
- π₁₀⁽ʲ⁾: P(Y₁ = 1, Y₂ = 0) in group j  
- π₀₁⁽ʲ⁾: P(Y₁ = 0, Y₂ = 1) in group j
- π₀₀⁽ʲ⁾: P(Y₁ = 0, Y₂ = 0) in group j

**Marginal probabilities**:
$$p_{j1} = \pi_{11}^{(j)} + \pi_{10}^{(j)}$$
$$p_{j2} = \pi_{11}^{(j)} + \pi_{01}^{(j)}$$

### Correlation Structure

The **tetrachoric correlation** ρⱼ measures association between binary endpoints by assuming underlying bivariate normal:

$$\pi_{11}^{(j)} = \Phi_2(\Phi^{-1}(p_{j1}), \Phi^{-1}(p_{j2}) \mid \rho_j)$$

where Φ₂ is the bivariate normal CDF.

**Correlation constraints**: Valid correlations must satisfy:
$$\max(0, p_{j1} + p_{j2} - 1) \leq \pi_{11}^{(j)} \leq \min(p_{j1}, p_{j2})$$

### Hypothesis Testing

**For endpoint k**:
$$H_{0k}: p_{1k} = p_{2k} \text{ vs. } H_{1k}: p_{1k} > p_{2k}$$

**For co-primary endpoints** (intersection-union test):
$$H_0 = H_{01} \cup H_{02} \text{ vs. } H_1 = H_{11} \cap H_{12}$$

**Decision rule**: Reject H₀ at level α if and only if **both** H₀₁ and H₀₂ are rejected.

## Available Exact Tests

### 1. Fisher's Exact Test (Conditional)

Fisher's exact test is a **conditional test** that conditions on the marginal totals.

**Test statistic**: Number of successes in treatment group for endpoint k:
$$X_k = \sum_{i=1}^{n_1} Y_{i1k}$$

**Conditional distribution** given marginals:
$$P(X_k = x_k \mid n_{1k}, n_{2k}, m_k) = \frac{\binom{m_k}{x_k}\binom{n_1 + n_2 - m_k}{n_1 - x_k}}{\binom{n_1 + n_2}{n_1}}$$

where:
- n₁ₖ = treatment group successes
- n₂ₖ = control group successes  
- mₖ = n₁ₖ + n₂ₖ = total successes

**P-value**: 
$$p\text{-value}_k = P(X_k \geq x_k^{\text{obs}} \mid m_k)$$

**Advantages**:
- Guarantees exact Type I error control
- Widely accepted and implemented
- Simple interpretation

**Disadvantages**:
- Conditional nature reduces power
- Conservative (actual α < nominal α)
- Fixed marginals may not reflect alternative hypothesis

### 2. Z-pooled Exact Unconditional Test

This is an **unconditional test** based on the pooled variance Z-statistic.

**Test statistic**:
$$Z_k = \frac{\hat{p}_{1k} - \hat{p}_{2k}}{\sqrt{\bar{p}_k(1-\bar{p}_k)(1/n_1 + 1/n_2)}}$$

where $\bar{p}_k = (n_1\hat{p}_{1k} + n_2\hat{p}_{2k})/(n_1 + n_2)$ is the pooled proportion.

**P-value calculation**:
The p-value is computed by maximizing over the nuisance parameter space:

$$p\text{-value}_k = \sup_{p_{1k}=p_{2k}=p_k} P(Z_k(\tilde{X}_k) \geq Z_k(x_k^{\text{obs}}) \mid p_k)$$

where the supremum is taken over all possible values of the common success probability under H₀.

**Computational approach**:
1. For each possible value of p under H₀ (grid search)
2. Calculate probability of observing test statistic as extreme or more extreme
3. Take maximum over all p values

**Advantages**:
- Unconditional (more powerful than Fisher's test)
- Exact Type I error control
- Corresponds to Barnard's test

**Disadvantages**:
- Computationally intensive
- Requires optimization over nuisance parameter

### 3. Boschloo's Exact Unconditional Test

Boschloo's test is an **exact unconditional test** that combines Fisher's p-value with unconditional maximization.

**Test procedure**:
1. Compute Fisher's exact p-value for observed data: p_F(x)
2. Maximize over nuisance parameter:

$$p\text{-value}_k = \sup_{p_{1k}=p_{2k}=p_k} P(p_F(\tilde{X}_k) \leq p_F(x_k^{\text{obs}}) \mid p_k)$$

**Key insight**: Rather than rejecting when Fisher's p-value is small, Boschloo rejects when the probability of obtaining a Fisher p-value as small or smaller (under H₀) is small.

**Advantages**:
- **Uniformly most powerful** among exact unconditional tests
- More powerful than Fisher's test
- Exact Type I error control

**Disadvantages**:
- Most computationally intensive
- Requires double summation (Fisher p-value + unconditional)

### Comparison of Test Methods

| Test | Type | Power | Computation | Type I Error |
|------|------|-------|-------------|--------------|
| Fisher | Conditional | Lowest | Fast | Conservative |
| Z-pooled | Unconditional | Medium | Moderate | Exact |
| Boschloo | Unconditional | Highest | Slow | Exact |

**Recommendation**: Use Boschloo's test for best power when computational resources permit. Use Fisher's test for quick calculations or when conservatism is acceptable.

## Joint Distribution and Power

### Copula-Based Dependence Model

For co-primary endpoints, we model the joint distribution using **copulas**. A copula C(u₁, u₂) describes the dependence structure between uniform random variables:

$$P(Y_1 \leq y_1, Y_2 \leq y_2) = C(F_1(y_1), F_2(y_2))$$

For binary endpoints with tetrachoric correlation ρ, we use the **Gaussian copula**:

$$C(u_1, u_2 \mid \rho) = \Phi_2(\Phi^{-1}(u_1), \Phi^{-1}(u_2) \mid \rho)$$

This gives the joint cell probabilities π₁₁, π₁₀, π₀₁, π₀₀.

### Exact Power Calculation

The exact power for co-primary endpoints is:

$$1 - \beta = P(\text{Reject } H_{01} \text{ and Reject } H_{02} \mid H_1)$$

This requires:
1. **Enumerate all possible outcomes**: For each (x₁₁, x₂₁, x₁₂, x₂₂) where xⱼₖ = number of successes in group j for endpoint k
2. **Calculate probability** under H₁ using copula model
3. **Determine rejection** based on exact test
4. **Sum probabilities** of all outcomes leading to rejection of both nulls

**Computational formula**:
$$1 - \beta = \sum_{\mathcal{R}} P(X_{11}=x_{11}, X_{21}=x_{21}, X_{12}=x_{12}, X_{22}=x_{22} \mid H_1)$$

where $\mathcal{R}$ is the rejection region (both tests reject).

### Sample Size Search Algorithm

Exact sample size calculation uses an **iterative search**:

1. **Initial guess**: Use asymptotic formula as starting point
2. **Evaluate power**: Calculate exact power for current n
3. **Refine**: 
   - If power < target, increase n
   - If power > target, decrease n
4. **Iterate**: Continue until power ≥ target
5. **Return**: Smallest n achieving target power

## Replicating Homma and Yoshida (2025) Table 4

Table 4 shows sample sizes for different scenarios and test methods.

### Scenario 1: Moderate Effect Sizes

```{r table4_scenario1}
# Scenario: p11=0.7, p21=0.4, p12=0.6, p22=0.3
rho_values <- c(0, 0.3, 0.5, 0.8)
tests <- c("Fisher", "Z-pool", "Boschloo")

results_scenario1 <- lapply(tests, function(test_name) {
  results_rho <- lapply(rho_values, function(rho) {
    result <- ss2BinaryExact(
      p11 = 0.7, p21 = 0.4,
      p12 = 0.6, p22 = 0.3,
      r = 1,
      rho1 = rho, rho2 = rho,
      alpha = 0.025,
      beta = 0.2,
      Test = test_name
    )
    
    power <- power2BinaryExact(
      n1 = result$n1, n2 = result$n2,
      p11 = 0.7, p21 = 0.4,
      p12 = 0.6, p22 = 0.3,
      rho1 = rho, rho2 = rho,
      alpha = 0.025,
      Test = test_name
    )
    
    data.frame(
      Test = test_name,
      rho = rho,
      n_per_group = result$n2,
      N_total = result$N,
      power1 = power$power1,
      power2 = power$power2,
      power_both = power$powerCoprimary
    )
  })
  bind_rows(results_rho)
})

table4_scenario1 <- bind_rows(results_scenario1)

# Display sample sizes
table4_scenario1_ss <- table4_scenario1 %>%
  select(Test, rho, n_per_group) %>%
  pivot_wider(
    names_from = rho,
    values_from = n_per_group,
    names_prefix = "rho_"
  )

kable(table4_scenario1_ss,
      caption = "Table 4 (Scenario 1): Sample Size per Group (p11=0.7, p21=0.4, p12=0.6, p22=0.3)",
      digits = 0,
      col.names = c("Test", "ρ=0", "ρ=0.3", "ρ=0.5", "ρ=0.8"))

# Display achieved power
table4_scenario1_power <- table4_scenario1 %>%
  select(Test, rho, power_both) %>%
  pivot_wider(
    names_from = rho,
    values_from = power_both,
    names_prefix = "rho_"
  )

kable(table4_scenario1_power,
      caption = "Achieved Power (Both Endpoints)",
      digits = 3,
      col.names = c("Test", "ρ=0", "ρ=0.3", "ρ=0.5", "ρ=0.8"))
```

**Observations**:
- Boschloo's test requires smallest sample size (most powerful)
- Fisher's test requires largest sample size (most conservative)
- Higher correlation reduces sample size across all tests
- All tests achieve target power ≥ 0.80

### Scenario 2: Larger Effect Sizes

```{r table4_scenario2}
# Note: Max feasible rho for this combination is ~0.76
rho_values_scenario2 <- c(0, 0.3, 0.5)

results_scenario2 <- lapply(tests, function(test_name) {
  results_rho <- lapply(rho_values_scenario2, function(rho) {
    result <- ss2BinaryExact(
      p11 = 0.8, p21 = 0.5,
      p12 = 0.7, p22 = 0.4,
      r = 1,
      rho1 = rho, rho2 = rho,
      alpha = 0.025,
      beta = 0.2,
      Test = test_name
    )
    
    power <- power2BinaryExact(
      n1 = result$n1, n2 = result$n2,
      p11 = 0.8, p21 = 0.5,
      p12 = 0.7, p22 = 0.4,
      rho1 = rho, rho2 = rho,
      alpha = 0.025,
      Test = test_name
    )
    
    data.frame(
      Test = test_name,
      rho = rho,
      n_per_group = result$n2,
      power_both = power$powerCoprimary
    )
  })
  bind_rows(results_rho)
})

table4_scenario2 <- bind_rows(results_scenario2)

table4_scenario2_wide <- table4_scenario2 %>%
  pivot_wider(
    names_from = rho,
    values_from = c(n_per_group, power_both),
    names_glue = "{.value}_rho{rho}"
  )

kable(table4_scenario2_wide,
      caption = "Table 4 (Scenario 2): p11=0.8, p21=0.5, p12=0.7, p22=0.4 (ρ ≤ 0.5)",
      digits = 3,
      col.names = c("Test", "n (ρ=0)", "n (ρ=0.3)", "n (ρ=0.5)", "Power (ρ=0)", "Power (ρ=0.3)", "Power (ρ=0.5)"))
```

**Key finding**: Larger effect sizes require smaller sample sizes, but correlation constraints become more restrictive (max ρ ≈ 0.76).

### Scenario 3: Unbalanced Allocation

```{r table4_scenario3}
# 2:1 allocation (treatment:control)
results_scenario3 <- lapply(tests, function(test_name) {
  results_rho <- lapply(c(0, 0.5), function(rho) {
    result <- ss2BinaryExact(
      p11 = 0.7, p21 = 0.4,
      p12 = 0.6, p22 = 0.3,
      r = 2,
      rho1 = rho, rho2 = rho,
      alpha = 0.025,
      beta = 0.2,
      Test = test_name
    )
    
    power <- power2BinaryExact(
      n1 = result$n1, n2 = result$n2,
      p11 = 0.7, p21 = 0.4,
      p12 = 0.6, p22 = 0.3,
      rho1 = rho, rho2 = rho,
      alpha = 0.025,
      Test = test_name
    )
    
    data.frame(
      Test = test_name,
      rho = rho,
      n_control = result$n2,
      n_treatment = result$n1,
      N_total = result$N,
      power_both = power$powerCoprimary
    )
  })
  bind_rows(results_rho)
})

table4_scenario3 <- bind_rows(results_scenario3)

kable(table4_scenario3,
      caption = "Table 4 (Scenario 3): Unbalanced 2:1 Allocation",
      digits = 3,
      col.names = c("Test", "ρ", "n (Control)", "n (Treatment)", "N (Total)", "Power (Both)"))
```

**Finding**: 2:1 allocation increases total sample size but maintains power. The percentage increase is consistent with balanced design.

## Comparison: Exact vs Asymptotic Methods

### Sample Size Comparison

```{r exact_vs_asymptotic}
# Compare exact and asymptotic methods with simplified scenarios
# Using larger effect sizes for faster exact computation
scenarios <- data.frame(
  p11 = c(0.70, 0.75),
  p21 = c(0.50, 0.55),
  p12 = c(0.65, 0.70),
  p22 = c(0.45, 0.50),
  rho = c(0, 0.5)
)

comparison_results <- lapply(1:nrow(scenarios), function(i) {
  s <- scenarios[i, ]
  
  # Exact (Boschloo)
  exact <- ss2BinaryExact(
    p11 = s$p11, p21 = s$p21,
    p12 = s$p12, p22 = s$p22,
    r = 1,
    rho1 = s$rho, rho2 = s$rho,
    alpha = 0.025,
    beta = 0.2,
    Test = "Boschloo"
  )
  
  # Asymptotic
  asymptotic <- ss2BinaryApprox(
    p11 = s$p11, p21 = s$p21,
    p12 = s$p12, p22 = s$p22,
    r = 1,
    rho1 = s$rho, rho2 = s$rho,
    alpha = 0.025,
    beta = 0.2,
    Test = "AN"
  )
  
  data.frame(
    Scenario = paste0("(", s$p11, ",", s$p21, "),(", s$p12, ",", s$p22, ")"),
    rho = s$rho,
    Exact = exact$n2,
    Asymptotic = asymptotic$n2,
    Difference = exact$n2 - asymptotic$n2,
    Pct_diff = round((exact$n2 - asymptotic$n2) / asymptotic$n2 * 100, 1)
  )
})

comparison_table <- bind_rows(comparison_results)

kable(comparison_table,
      caption = "Comparison: Exact (Boschloo) vs Asymptotic Methods",
      col.names = c("Scenario", "ρ", "Exact", "Asymptotic", "Diff", "% Diff"))
```

**Interpretation**: 
- Exact methods typically require slightly larger sample sizes
- Difference is small for moderate effect sizes and probabilities
- Difference increases for smaller sample sizes or extreme probabilities

### Type I Error Rate Comparison

```{r type_i_error_simulation, eval=FALSE}
# Conceptual code (not run in vignette)
# Simulate Type I error rates under null hypothesis

n <- 50  # Small sample size
p1 <- 0.7
p2 = 0.7  # Null: equal probabilities
nsim <- 10000

# For each simulation:
# 1. Generate data under H0 (p1 = p2)
# 2. Apply exact test
# 3. Apply asymptotic test
# 4. Record rejections

# Expected result:
# - Exact test: empirical α ≤ 0.025 (conservative)
# - Asymptotic test: empirical α may exceed 0.025 (anti-conservative for small n)
```

**Key insight**: Exact methods guarantee Type I error control even for small samples, while asymptotic methods may be anti-conservative.

## Power Curves

### Power vs Sample Size

```{r power_curves, fig.width=8, fig.height=6}
# Fixed scenario
p11 <- 0.7
p21 <- 0.4
p12 <- 0.6
p22 <- 0.3
rho <- 0.5

# Sample size range
n_seq <- seq(30, 80, by = 5)

# Calculate power for each test method
power_by_test <- lapply(tests, function(test_name) {
  powers <- sapply(n_seq, function(n) {
    pw <- power2BinaryExact(
      n1 = n, n2 = n,
      p11 = p11, p21 = p21,
      p12 = p12, p22 = p22,
      rho1 = rho, rho2 = rho,
      alpha = 0.025,
      Test = test_name
    )
    pw$powerCoprimary
  })
  data.frame(n = n_seq, power = powers, test = test_name)
})

power_curves_data <- bind_rows(power_by_test)

# Plot
library(ggplot2)
ggplot(power_curves_data, aes(x = n, y = power, color = test)) +
  geom_line(linewidth = 1.2) +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "gray") +
  labs(
    title = "Power vs Sample Size: Comparison of Exact Tests",
    subtitle = paste0("p11=", p11, ", p21=", p21, ", p12=", p12, ", p22=", p22, ", ρ=", rho),
    x = "Sample Size per Group (n)",
    y = "Power (Both Endpoints)",
    color = "Test Method"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Observation**: Boschloo's test achieves target power with smallest sample size, confirming it is the most powerful exact test.

## Clinical Application: Small Oncology Trial

Consider a small Phase II oncology trial with:
1. **Tumor response** (CR/PR): Yes/No
2. **Symptom control**: Yes/No

```{r oncology_example}
# Design parameters - using larger effect sizes for faster computation
# Expected response rates with treatment: 50% response, 65% symptom control
# Expected rates with control: 25% response, 40% symptom control
# Small trial: exact methods appropriate
# Moderate correlation: ρ = 0.3 (reduced for feasibility)

oncology_design <- ss2BinaryExact(
  p11 = 0.50,           # Response (treatment)
  p21 = 0.25,           # Response (control)
  p12 = 0.65,           # Symptom control (treatment)
  p22 = 0.40,           # Symptom control (control)
  r = 1,
  rho1 = 0.3, rho2 = 0.3,
  alpha = 0.025,
  beta = 0.2,
  Test = "Boschloo"     # Most powerful
)

cat("Oncology Trial Design (Exact Method):\n")
cat("Sample size per group:", oncology_design$n2, "\n")
cat("Total sample size:", oncology_design$N, "\n\n")

# Verify power
oncology_power <- power2BinaryExact(
  n1 = oncology_design$n1,
  n2 = oncology_design$n2,
  p11 = 0.50, p21 = 0.25,
  p12 = 0.65, p22 = 0.40,
  rho1 = 0.3, rho2 = 0.3,
  alpha = 0.025,
  Test = "Boschloo"
)

cat("Power verification:\n")
cat("Power for response:", round(oncology_power$power1, 3), "\n")
cat("Power for symptom control:", round(oncology_power$power2, 3), "\n")
cat("Power for both (co-primary):", round(oncology_power$powerCoprimary, 3), "\n\n")

# Compare with asymptotic method
oncology_asymptotic <- ss2BinaryApprox(
  p11 = 0.50, p21 = 0.25,
  p12 = 0.65, p22 = 0.40,
  r = 1,
  rho1 = 0.3, rho2 = 0.3,
  alpha = 0.025,
  beta = 0.2,
  Test = "AN"
)

cat("Comparison with asymptotic method:\n")
cat("Asymptotic sample size per group:", oncology_asymptotic$n2, "\n")
cat("Exact sample size per group:", oncology_design$n2, "\n")
cat("Difference:", oncology_design$n2 - oncology_asymptotic$n2, "\n")
```

## Summary

### Key Findings

1. **Test ranking by power**: Boschloo > Z-pooled > Fisher

2. **Conservatism**: All exact tests maintain Type I error ≤ α, but Fisher is most conservative

3. **Correlation impact**: ~5-7% reduction at ρ = 0.5, ~10% at ρ = 0.8 (similar to asymptotic)

4. **Exact vs asymptotic**: Exact methods require slightly larger samples but guarantee Type I error control

5. **Computational cost**: Boschloo > Z-pooled > Fisher

### When to Use Each Test

**Fisher's exact test**:
- Quick calculations needed
- Conservative approach acceptable
- Widely recognized/required by regulators

**Z-pooled exact test**:
- Balance between power and computation
- Unconditional inference desired
- Moderate computational resources

**Boschloo's exact test**:
- Maximum power required
- Computational resources available
- Small sample size (every subject counts)

### Practical Recommendations

1. **Sample size < 100**: Always use exact methods

2. **Extreme probabilities** (p < 0.1 or p > 0.9): Use exact methods regardless of sample size

3. **Regulatory submissions**: Check requirements; exact methods often preferred

4. **Power priority**: Use Boschloo's test if computational time permits

5. **Conservative approach**: Use Fisher's test for added safety margin

6. **Sensitivity analysis**: Calculate for multiple test methods and correlations

### Computational Considerations

- **Fisher**: Milliseconds per calculation
- **Z-pooled**: Seconds per calculation
- **Boschloo**: Minutes per calculation (for small n)

Modern computers handle all three tests efficiently for typical clinical trial sample sizes (n < 200).

## References

Homma, G., & Yoshida, T. (2025). Exact power and sample size in clinical trials with two co-primary binary endpoints. *Statistical Methods in Medical Research* (in press).

Boschloo, R. D. (1970). Raised conditional level of significance for the 2 × 2-table when testing the equality of two probabilities. *Statistica Neerlandica*, 24(1), 1-9.

Barnard, G. A. (1947). Significance tests for 2 × 2 tables. *Biometrika*, 34(1/2), 123-138.

Fisher, R. A. (1935). The logic of inductive inference. *Journal of the Royal Statistical Society*, 98(1), 39-82.

Mehrotra, D. V., Chan, I. S., & Berger, R. L. (2003). A cautionary note on exact unconditional inference for a difference between two independent binomial proportions. *Biometrics*, 59(2), 441-450.
